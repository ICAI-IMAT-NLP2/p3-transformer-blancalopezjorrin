{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrating Different Decoding Strategies with Hugging Face Transformers\n",
    "In this notebook, we will explore various decoding strategies for text generation using a small encoder-decoder model from Hugging Face's Transformers library. We'll apply these strategies to two different datasets:\n",
    "\n",
    "- Translation task where deterministic strategies are expected to perform better.\n",
    "- Summarization task where stochastic strategies might yield more diverse and informative outputs.\n",
    "\n",
    "The decoding strategies we'll test include:\n",
    "\n",
    "1. Greedy Search\n",
    "2. Beam Search\n",
    "3. Temperature Sampling\n",
    "4. Top-k Sampling\n",
    "5. Top-p (Nucleus) Sampling\n",
    "\n",
    "We'll define a custom ```generate_text``` function to apply these strategies and evaluate their performance using appropriate metrics for each dataset.\n",
    "\n",
    "Here are some useful links you might want to check:\n",
    "- [Auto Classes](https://huggingface.co/docs/transformers/model_doc/auto)\n",
    "- [transformers\\AutoModel](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModel)\n",
    "- [transformers\\AutoTokenizer](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoTokenizer)\n",
    "- [Google's T5](https://arxiv.org/pdf/1910.10683)\n",
    "- [T5-small](https://huggingface.co/google-t5/t5-small)\n",
    "- [Flan-T5-small](https://huggingface.co/google/flan-t5-small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets sacrebleu rouge_score evaluate --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Pre-trained Model and Tokenizer\n",
    "\n",
    "We'll use the ```flan-T5-small``` model, which is a small encoder-decoder model suitable for both story_gen and summarization tasks. This model is based on Google's ```t5-small``` model, but fine-tuned on more than 1000 additional tasks covering also more languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google/flan-t5-small'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(input_text, strategy='greedy', max_length=50, **kwargs):\n",
    "    \"\"\"Generates text based on the specified decoding strategy.\n",
    "\n",
    "    Args:\n",
    "        input_text (str): The input text to be processed by the model.\n",
    "        strategy (str, optional): The decoding strategy to use. Defaults to 'greedy'.\n",
    "            Options include:\n",
    "            - 'greedy': Greedy search decoding.\n",
    "            - 'beam': Beam search decoding.\n",
    "            - 'temperature': Temperature sampling.\n",
    "            - 'top-k': Top-k sampling.\n",
    "            - 'top-p': Top-p (nucleus) sampling.\n",
    "            - 'contrastive': Contrastive search decoding.\n",
    "        max_length (int, optional): The maximum length of the generated text. Defaults to 50.\n",
    "        **kwargs: Additional keyword arguments specific to the decoding strategy.\n",
    "\n",
    "    Keyword Args:\n",
    "        num_beams (int, optional): Number of beams for beam search. Defaults to 5.\n",
    "            Used when `strategy='beam'`.\n",
    "        temperature (float, optional): Sampling temperature. Defaults to 1.0.\n",
    "            Used when `strategy='temperature'`.\n",
    "        top_k (int, optional): The number of highest probability vocabulary tokens to keep for top-k filtering. Defaults to 50.\n",
    "            Used when `strategy='top-k'` or when `strategy='contrastive'`.\n",
    "        top_p (float, optional): Cumulative probability for nucleus sampling. Defaults to 0.95.\n",
    "            Used when `strategy='top-p'`.\n",
    "        penalty_alpha (float, optional): Contrastive search penalty factor. Defaults to 0.6.\n",
    "            Used when `strategy='contrastive'`.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated text based on the decoding strategy.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an unknown decoding strategy is specified.\n",
    "    \"\"\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if strategy == 'greedy':\n",
    "            output_ids = model.generate(input_ids, max_length=max_length)\n",
    "        elif strategy == 'beam':\n",
    "            num_beams = kwargs.get('num_beams', 5)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, num_beams=num_beams, early_stopping=True\n",
    "            )\n",
    "        elif strategy == 'temperature':\n",
    "            temperature = kwargs.get('temperature', 1.0)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, do_sample=True, temperature=temperature\n",
    "            )\n",
    "        elif strategy == 'top-k':\n",
    "            top_k = kwargs.get('top_k', 50)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, do_sample=True, top_k=top_k\n",
    "            )\n",
    "        elif strategy == 'top-p':\n",
    "            top_p = kwargs.get('top_p', 0.95)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, do_sample=True, top_p=top_p\n",
    "            )\n",
    "        elif strategy == 'contrastive':\n",
    "            penalty_alpha = kwargs.get('penalty_alpha', 0.6)\n",
    "            top_k = kwargs.get('top_k', 4)\n",
    "            output_ids = model.generate(\n",
    "                input_ids, max_length=max_length, penalty_alpha=penalty_alpha, top_k=top_k\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Unknown strategy: {}\".format(strategy))\n",
    "\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    return output_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case 1: Neural Machine Translation\n",
    "The WMT16 English-German dataset is a collection of parallel sentences in English and German used for machine translation tasks. It is part of the Conference on Machine story_gen (WMT) shared tasks, which are benchmarks for evaluating machine story_gen systems. The dataset contains professionally translated sentences and covers a variety of topics, making it ideal for training and evaluating story_gen models.\n",
    "\n",
    "We'll use a subset (1% of the test split) of the dataset for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_nmt = load_dataset('wmt16', 'de-en', split='test[:1%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text:\n",
      "Das Verhältnis zwischen Obama und Netanyahu ist seit Jahren gespannt.\n",
      "\n",
      "Target Text:\n",
      "Relations between Obama and Netanyahu have been strained for years.\n"
     ]
    }
   ],
   "source": [
    "# Randomly select a sample from the dataset\n",
    "sample = random.choice(dataset_nmt['translation'])\n",
    "input_text = sample['de']\n",
    "target_text = sample['en']\n",
    "\n",
    "print(\"Input Text:\")\n",
    "print(input_text)\n",
    "print(\"\\nTarget Text:\")\n",
    "print(target_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to preprocess the dataset to prepare it for input into the T5 model. The T5 model expects input in a specific format, including a task prefix. This is because the T5 model is a multi-purpose model, being able to perform several task, so we need to tell it what it must do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_translation(examples: list[dict]) -> dict:\n",
    "    \"\"\"Preprocesses a single example for the translation task.\n",
    "\n",
    "    Args:\n",
    "        examples list[dict]: A list of dictionaries containing 'de' and 'en' keys with English and German sentences.\n",
    "\n",
    "    Returns:\n",
    "        dict[list]: A dictionary of lists with added 'src_texts' and 'tgt_texts' keys for model input and target.\n",
    "\n",
    "    The function:\n",
    "    - Adds the task prefix 'translate German to English: ' to the German sentence.\n",
    "    - Stores the result in 'src_texts'.\n",
    "    - Copies the English sentence to 'tgt_texts'.\n",
    "    \"\"\"\n",
    "    # Empty dictionary\n",
    "    texts = {}\n",
    "    \n",
    "    # T5 expects a \"translate German to English: \" prefix\n",
    "    texts['src_texts'] = ['translate German to English: ' + ex['de'] for ex in examples]\n",
    "    texts['tgt_texts'] = [ex['en'] for ex in examples]\n",
    "    return texts\n",
    "\n",
    "dataset_nmt_preproc = preprocess_translation(dataset_nmt[\"translation\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can generate a translation using the t5 model. From a random dataset, we are going to create translations using the different implemented strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translate German to English: Innerhalb des Hauses fanden die Beamten die Leiche von Amy Prentiss und eine handgeschriebene Notiz, die auf einen weißen Block gekritzelt war: \"Mir tut es so leid, ich wollte, ich könnte es rückgängig machen, ich liebte Amy und sie ist die einzige Frau, die mich jemals liebte\". Dies stand nach Angaben der Behörden in dem Brief, und er war von Lamb unterzeichnet.\n",
      "Original translation: \n",
      "Inside the home, officers found Amy Prentiss' body and a hand-written note scribbled on a white legal pad: \"I am so very sorry I wish I could take it back I loved Amy and she is the only woman who ever loved me,\" read the letter authorities say was signed by Lamb.\n",
      "Greedy Search: \n",
      "In the House, the officers of Amy Prentiss and a handwritten note, which was on a white block, said, \"I would like to make it a bit more difficult, I would like to make it more\n",
      "Beam Search: \n",
      "Within the house, the officers of Amy Prentiss and a handwritten note, which was written on a white block, said, \"Mirrrrrrrrrrrrrrrr\n",
      "Temperature Sampling: \n",
      "Within the house, the officers of Amy Prentiss and a handwritten note that was issued on a black block were: \"I wanted to make it look like that, I lacked Amy and she is the only woman\n",
      "Top-k Sampling: \n",
      "Throughout the House they found the officers, but on one hand, were very poor, and a copy of a note that had been written and issued on a dark block known as the Leiche of Amy Prentiss\n",
      "Top-p Sampling: \n",
      "In the absence of authorities, the officers made the note of Amy Prentiss and a handwritten note, which had the right logo of a black card. But I wondered what it might be like, say, 'M\n"
     ]
    }
   ],
   "source": [
    "# Obtain source and target texts\n",
    "random_index = random.randint(0, len(dataset_nmt_preproc['src_texts']))\n",
    "random_src_sentence = dataset_nmt_preproc['src_texts'][random_index]\n",
    "random_tgt_sentence = dataset_nmt_preproc['tgt_texts'][random_index]\n",
    "\n",
    "# Obtain the translated sentence\n",
    "translation_greedy= generate_text(random_src_sentence, strategy='greedy')\n",
    "translation_beam_search = generate_text(random_src_sentence, strategy='beam', num_beams=5)\n",
    "translation_temperature = generate_text(random_src_sentence, strategy='temperature', temperature=0.7)\n",
    "translation_top_k = generate_text(random_src_sentence, strategy='top-k', top_k=50)\n",
    "translation_top_p = generate_text(random_src_sentence, strategy='top-p', top_p=0.95)\n",
    "# translation_contrastive = generate_text(random_src_sentence, strategy='contrastive', top_k=4, penalty_alpha=0.6)\n",
    "\n",
    "print(random_src_sentence)\n",
    "print(f\"Original translation: \\n{random_tgt_sentence}\")\n",
    "print(f\"Greedy Search: \\n{translation_greedy}\")\n",
    "print(f\"Beam Search: \\n{translation_beam_search}\")\n",
    "print(f\"Temperature Sampling: \\n{translation_temperature}\")\n",
    "print(f\"Top-k Sampling: \\n{translation_top_k}\")\n",
    "print(f\"Top-p Sampling: \\n{translation_top_p}\")\n",
    "# print(f\"Contrastive Search: \\n{translation_contrastive}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do you see something different between the deterministic and the stochastic strategies? Try different random sentences.\n",
    ">>> From what I could observe, deterministic strategies are usually more fluent but can be overly literal or rigid, sometimes failing on proper nouns or less common phrases. On the other hand, stochastic strategies inject variability. They can generate diverse outputs, but at the cost of accuracy and fidelity to the original sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metric: BLEU Score\n",
    "#### What is BLEU Score?\n",
    "The BLEU (Bilingual Evaluation Understudy) score is a metric for evaluating the quality of text that has been machine-translated from one language to another. It compares a candidate translation to one or more reference translations and calculates a score based on the overlap of n-grams (contiguous sequences of words).\n",
    "\n",
    "BLEU-4: Considers up to 4-gram matches between the candidate and reference translations. It provides a balance between precision (matching words) and fluency (maintaining the structure of the language). Using BLEU-4 allows us to capture not just individual word matches (unigrams) but also phrases of up to four words. This makes the evaluation more sensitive to the quality of the translation in terms of both accuracy and fluency.\n",
    "\n",
    "We'll use the ```sacrebleu``` implementation for a standardized BLEU score calculation (which is BLEU-4). You can check the details [here](https://aclanthology.org/W14-3346.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_metric = evaluate.load('sacrebleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating strategy: greedy\n",
      "BLEU-4 score for greedy: 20.99\n",
      "Evaluating strategy: beam\n",
      "BLEU-4 score for beam: 21.14\n",
      "Evaluating strategy: temperature\n",
      "BLEU-4 score for temperature: 11.88\n",
      "Evaluating strategy: top-k\n",
      "BLEU-4 score for top-k: 12.01\n",
      "Evaluating strategy: top-p\n",
      "BLEU-4 score for top-p: 12.20\n"
     ]
    }
   ],
   "source": [
    "strategies = ['greedy', 'beam', 'temperature', 'top-k', 'top-p'] # , 'contrastive']\n",
    "results = {}\n",
    "\n",
    "hyperparameters = {\n",
    "    'num_beams': 5,\n",
    "    'temperature': 1.0,\n",
    "    'top_k': 50,\n",
    "    'top_p': 0.95,\n",
    "    'penalty_alpha': 0.6\n",
    "}\n",
    "\n",
    "for strategy in strategies:\n",
    "    print(f\"Evaluating strategy: {strategy}\")\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for src_text, tgt_text in zip(dataset_nmt_preproc[\"src_texts\"], dataset_nmt_preproc[\"tgt_texts\"]): \n",
    "        pred = generate_text(src_text, strategy=strategy, **hyperparameters)\n",
    "        predictions.append(pred)\n",
    "        references.append([tgt_text])  # SacreBLEU expects a list of references\n",
    "\n",
    "    # Compute BLEU-4 score\n",
    "    bleu = bleu_metric.compute(predictions=predictions, references=references, smooth_method='exp')\n",
    "    results[strategy] = bleu['score']\n",
    "    print(f\"BLEU-4 score for {strategy}: {bleu['score']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Seeing the above translations and the BLEU score of the different strategies, which strategy would you choose for this use case?\n",
    ">>> Deterministic strategies (Greedy, Beam) clearly outperform stochastic strategies (Temperature, Top-k, Top-p) in BLEU score, meaning their translations are much closer to the reference. Beam search is slightly better than Greedy, giving the most accurate and fluent translation. Stochastic methods produce more variety but at a big cost in fidelity. In conclusion, I would choose beam search, as it maximizes accuracy and preserves the meaning, which is crucial in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case 2: Story generation\n",
    "\n",
    "The ```WritingPrompts``` dataset is a collection of imaginative prompts and corresponding stories from the Reddit community. It contains over 300,000 stories written in response to various prompts, making it suitable for training and evaluating models on creative text generation tasks.\n",
    "\n",
    "We'll use a subset of the dataset for evaluation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_st_gen = load_dataset('llm-aes/writing-prompts', split='train[:1%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Text:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Randomly select a sample from the dataset\n",
    "sample = random.choice(dataset_st_gen['prompt'])\n",
    "\n",
    "print(\"Sample Text:\")\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to tell the t5 model to generate text after the input sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_story_generation(examples: list[dict]) -> dict:\n",
    "    \"\"\"Preprocesses a single example for the story generation task.\n",
    "\n",
    "    Args:\n",
    "        examples (list[dict]): A list of dictionaries containing 'prompt' key.\n",
    "\n",
    "    Returns:\n",
    "        dict[list]: A dictionary of list with added 'src_texts' for model input.\n",
    "\n",
    "    The function:\n",
    "    - Adds the task prefix 'Write a story based on: ' to the prompt.\n",
    "    \"\"\"\n",
    "    # Empty dictionary\n",
    "    texts = {}\n",
    "    \n",
    "    # T5 expects a task prefix\n",
    "    texts['src_texts'] = ['Write a story based on: ' + ex['prompt'] for ex in examples]\n",
    "    return texts\n",
    "\n",
    "dataset_st_gen_preproc = preprocess_story_generation(dataset_st_gen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have no original story to compare how good the model generates a story, you should compare the different decoding strategies by looking at some random stories: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a story based on:  A magic coin gives the owner just enough money to get by , until they can finally support themselves , then they must give it to another person in need .\n",
      "\n",
      "Greedy Search: \n",
      "The owner of a magic coin is a magician , and he is a magician . The owner is a magician , and he is a magician . The magic coin is a magic coin , and he is a magician . The owner is a magician , and he is a magician . The owner is a magician , and he is a magician . The owner is a magician , and he is a magician . The owner is a magician , and he is a magician . The owner is a magician , and he is a magician . The owner is a magician , and he is a magician . The owner is a magician , and he is a magician . The owner is a magician , and he is a magician . The owner is a magician , and he is a magician . The owner is a magician , and he is a magician . The owner is a magician , and he is a magician . The owner is a magician , and he is a magician .\n",
      "Beam Search: \n",
      "The owner of a magic coin gave the owner just enough money to get by , until they can finally support themselves , then they must give it to another person in need . A magic coin gives the owner just enough money to get by , until they can finally support themselves , then they must give it to another person in need .\n",
      "Temperature Sampling: \n",
      "The owner of a magic coin , in a fictional appearance , tries to donate the money that the owner needs . The owner is putting the money in the hands of the owner , but the owner is not willing to do anything . The money is being spent on the owner , a magic coin that the owner needs to pay for . It is one of the first magical coins , the ones he needs to make .\n",
      "Top-k Sampling: \n",
      "A magical coin handed down by one person will get their money ready . The owner is just as thrilled as the owner , an owner who has just enough money to buy all that money that the one-day owner could need . He gives the coin to a person at auction but will not be able to give the coin until it is ready .\n",
      "Top-p Sampling: \n",
      "The property owner bought its own magic coin for the first time . A magic coin has melted into a magic coin and a magic ball has become their favorite . Their best friends and parents and friends all use the magic ball to beat others out of their troubles .\n"
     ]
    }
   ],
   "source": [
    "# Obtain source and target texts\n",
    "random_index = random.randint(0, len(dataset_st_gen_preproc['src_texts']))\n",
    "random_src_sentence = dataset_st_gen_preproc['src_texts'][random_index]\n",
    "\n",
    "# Obtain the translated sentence\n",
    "story_gen_greedy= generate_text(random_src_sentence, max_length=300, strategy='greedy')\n",
    "story_gen_beam_search = generate_text(random_src_sentence, max_length=300, strategy='beam', num_beams=5)\n",
    "story_gen_temperature = generate_text(random_src_sentence, max_length=300, strategy='temperature', temperature=0.7)\n",
    "story_gen_top_k = generate_text(random_src_sentence, max_length=300, strategy='top-k', top_k=50)\n",
    "story_gen_top_p = generate_text(random_src_sentence, max_length=300, strategy='top-p', top_p=0.95)\n",
    "# story_gen_contrastive = generate_text(random_src_sentence, max_length=300, strategy='contrastive', top_k=4, penalty_alpha=0.6)\n",
    "\n",
    "print(random_src_sentence)\n",
    "print(f\"Greedy Search: \\n{story_gen_greedy}\")\n",
    "print(f\"Beam Search: \\n{story_gen_beam_search}\")\n",
    "print(f\"Temperature Sampling: \\n{story_gen_temperature}\")\n",
    "print(f\"Top-k Sampling: \\n{story_gen_top_k}\")\n",
    "print(f\"Top-p Sampling: \\n{story_gen_top_p}\")\n",
    "# print(f\"Contrastive Search: \\n{story_gen_contrastive}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Seeing the above generated stories of the different strategies, which strategy would you choose for this use case?\n",
    ">>> In this case, I would also choose beam search. It produces a coherent and accurate story that stays true to the original prompt, clearly explaining how the magic coin works. While it may be a bit plain, it avoids the repetition and confusion seen in Greedy, Temperature, Top-k, and Top-p outputs. The other strategies introduce creative elements or phrasing errors, but Beam search gives a readable and faithful narrative that can be easily expanded into a full story if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity Analysis\n",
    "\n",
    "Try different hyperparameter values for the decoding strategies, try to optimize the BLEU score for the Neural Machine Translation case and generate better stories in the second use case.\n",
    "\n",
    "- Which optimal configuration have you found for use case 1? Which are your conclusions based on your analysis?\n",
    ">>> For the Neural Machine Translation case, the optimal configuration was Beam Search with a moderate beam width. This setup achieved the highest BLEU-4 score (21.14) and produced translations that were faithful to the original text, fluent, and coherent. Greedy search was close but slightly lower in accuracy, while stochastic strategies like Temperature, Top-k, and Top-p added diversity but significantly reduced fidelity, leading to lower BLEU scores. The conclusion is that for translation tasks where accuracy is critical, deterministic strategies with beam search are preferable, while stochastic methods are more suited for creative or exploratory generation, not exact translation.\n",
    "\n",
    "- Which optimal configuration have you found for use case 2? Which are your conclusions bsaed on your analysis?\n",
    ">>> The optimal configuration was Beam Search with a moderate beam width. It produced a coherent, readable story that stayed true to the prompt about the magic coin. Greedy search was unusable due to repetitive output, while stochastic strategies like Temperature, Top-k, and Top-p introduced creativity but often produced confusing or irrelevant content. The conclusion is that for tasks requiring clarity and fidelity to a prompt, deterministic strategies like beam search work best, whereas stochastic sampling is better suited for generating creative variations that may need human refinement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
